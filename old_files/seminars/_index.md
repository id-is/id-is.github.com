---
title: "Seminars"
date: 2023-12-14T22:16:26+02:00
draft: false
theme_version: '2.8.2'
cascade:
  featured_image: '/backround.png'
description: It's a Weekly Session Where each Member of the group Presentate his Work
---

[2024](#2024) - [2023](#2023)

# 2024 {#2024}

- ## Ecowise Experiment (Ypatia Dami)

  [Presentation](/seminars/ecowise.pdf)

- ## Informed Machine Learning An Overview (Vassilis Gkatsis)

  [Presentation](/seminars/informedML.pdf)

# 2023 {#2023}

- ## Preference Representation using Higher-Order LP (Antonis Trompoukis)

  [Presentation](/seminars/preferences.pdf)

- ## Neuro Symbolic AI (Antonis Ganios)

  Abstract : In recent years, neural systems have demonstrated highly effective learning ability and superior perception intelligence. However, they have been found to lack effective reasoning and cognitive ability. On the other hand, symbolic systems exhibit exceptional cognitive intelligence but suffer from poor learning capabilities when compared to neural systems. Recognizing the advantages and disadvantages of both methodologies, an ideal solution emerges: combining neural systems and symbolic systems to create neural-symbolic learning systems that possess powerful perception and cognition. The purpose of this presentation is to illustrate different categories of neuro-symbolic models and frameworks related to them.

  [Presentation](/seminars/neurosymbolic.pdf)

- ## Physics (Vasileios Vatellis)

  Abstract : A brief introduction to the Standard Model (SM) and the utilization of two Higgs doublet models and their extension to explain SM's inconsistencies in neutrinos masses and flavor observables using Branco Grimus Lavoura (BGL) quark structure. Providing also new scalar particles and interesting topologies for collider physics

  [Presentation](/seminars/Physics.pdf)

- ## All About Transformers (Thodoris Aivalis)

  Abstract : The engagement with the field of artificial intelligence, which has seen significant development in recent times, dates back to the mid 20th century. Transformers first mentioned by the paper titled Attention is All you Need whichintroduced a model utilizing the attention mechanism to establish connections between data in order to make predictions. Since then, numerous and diverse Transformershave been created, building upon and extending the original model. The purpose of this presentation is to illustrate the applications and results of Trnasformers in various fields, displaying at the same time challenges and limtations of those models.

  [Presentation](/seminars/Tranformers.pdf)

[Back to Top](#top)
